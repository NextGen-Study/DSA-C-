
## Module 1: Algorithm Analysis 

### 1. Importance of Algorithms and Data Structures

**Algorithm**:
An algorithm is a finite sequence of steps that provides a solution to a problem. It takes some input, processes it, and produces an output.

**Data Structure**:
A data structure is a particular way of organizing and storing data so that it can be accessed and modified efficiently.

**Why are algorithms and data structures important?**

1. Efficiency: Good algorithms can perform tasks faster and use less memory.
2. Scalability: Efficient data structures and algorithms are necessary for handling large inputs.
3. Reusability: Once designed, algorithms can be reused for similar problems.
4. Maintainability: Structured data and well-thought algorithms result in clean and maintainable code.
5. Backbone of programming: They are essential in solving complex problems in software development, web applications, databases, operating systems, AI, etc.

**Example**:
To search a name in a telephone directory:

* Linear search will take O(n) time.
* Binary search will take O(log n) time.
  This shows how the algorithm choice affects performance.

---

### 2. Fundamentals of Algorithm Analysis

To analyze an algorithm, we measure:

1. Time complexity: How the runtime increases with input size.
2. Space complexity: How much memory is used as input grows.

#### Time Complexity

It estimates the number of operations required by the algorithm as a function of the input size.

* Best Case: Minimum time taken (ideal situation).
* Average Case: Expected time taken for typical inputs.
* Worst Case: Maximum time taken (safety upper bound).

#### Space Complexity

It estimates the total memory an algorithm needs:

* Input storage
* Auxiliary space (temporary variables, stacks, etc.)

**Example**:
For a function that reverses an array:

* Time complexity = O(n)
* Space complexity = O(1) if reversed in-place

---

### 3. Types of Asymptotic Notations and Orders of Growth

Asymptotic notations help describe the behavior of an algorithm for large inputs (as n → ∞). They provide a mathematical way to express time/space complexity.

#### Big-O Notation (O)

Represents the **upper bound** of an algorithm. It tells us the worst-case scenario.

Example:

* O(n): linear time
* O(n^2): quadratic time

Used for worst-case analysis.

#### Omega Notation (Ω)

Represents the **lower bound**. It gives the best-case scenario.

Example:

* Ω(n): at least linear time

#### Theta Notation (Θ)

Represents the **tight bound**. If an algorithm takes Θ(n), its runtime grows linearly both in best and worst cases.

Example:

* Θ(n log n): used for merge sort and heap sort.

#### Common Orders of Growth

* Constant: O(1)
* Logarithmic: O(log n)
* Linear: O(n)
* Linearithmic: O(n log n)
* Quadratic: O(n^2)
* Exponential: O(2^n)
* Factorial: O(n!)

As n increases, the time or space increases based on the growth rate. Lower complexity is better.

---

### 4. Algorithm Efficiency: Best, Worst, and Average Case

#### Best Case

* The fastest time the algorithm takes.
* Rarely used in practice, as it’s the ideal scenario.
* Denoted by Ω(n)

Example: In linear search, best case occurs when the element is at the first index.

#### Worst Case

* The longest time the algorithm might take.
* Very important for safety-critical systems.
* Denoted by O(n)

Example: In linear search, worst case is when the element is not present or is at the end.

#### Average Case

* The expected time for random input.
* Often calculated using probability and assumptions.
* Denoted by average-case complexity.

Example: In linear search, average case assumes the element is in the middle.

---

### 5. Analysis of Non-Recursive and Recursive Algorithms

#### Non-Recursive Algorithm Analysis

* Straightforward to analyze.
* Count the number of iterations and operations in loops.

Example:

```
for (int i = 0; i < n; i++) {
    sum = sum + i;
}
```

This loop runs n times → Time Complexity = O(n)

#### Recursive Algorithm Analysis

* Requires forming a recurrence relation based on function calls.
* Recurrence relation is then solved to find time complexity.

Example:

```
function fact(n) {
    if (n == 0) return 1;
    else return n * fact(n-1);
}
```

Recurrence: T(n) = T(n-1) + O(1)

Solving gives: T(n) = O(n)

Recursive algorithms are harder to analyze but are elegant and used in divide and conquer techniques.

---

### 6. Asymptotic Analysis for Recurrence Relations

To analyze recursive algorithms, we often use recurrence relations. A recurrence relation expresses the overall time complexity based on sub-problems.

There are four main methods to solve recurrence relations:

#### a) Iteration Method

* Expand the recurrence step-by-step
* Continue until the base case is reached

Example:
T(n) = T(n - 1) + 1

Expand:
T(n) = T(n - 2) + 2
T(n) = T(n - 3) + 3
...
T(n) = T(1) + (n - 1) = O(n)

#### b) Substitution Method

* Guess the time complexity (T(n) = O(f(n)))
* Use mathematical induction to prove it

Example:
T(n) = 2T(n/2) + n

Guess: T(n) = O(n log n)

Prove it using induction:
T(n) ≤ 2(n/2 log(n/2)) + n = n log n

Hence, T(n) = O(n log n)

#### c) Master Method

Used for divide and conquer algorithms of the form:
T(n) = aT(n/b) + f(n)

Where:

* a ≥ 1 and b > 1 are constants
* f(n) is an additional cost

Three cases:

1. If f(n) = O(n^log\_b(a - ε)) → T(n) = Θ(n^log\_b a)
2. If f(n) = Θ(n^log\_b a) → T(n) = Θ(n^log\_b a \* log n)
3. If f(n) = Ω(n^log\_b a + ε) and regularity condition holds → T(n) = Θ(f(n))

Example:
T(n) = 2T(n/2) + n → Here a=2, b=2, f(n)=n
log\_b a = log₂2 = 1
f(n) = Θ(n) → Case 2 → T(n) = Θ(n log n)

#### d) Recursive Tree Method

* Draw the recursion tree
* Sum costs at each level

Example:
T(n) = 2T(n/2) + n

Tree:

* Level 0: n
* Level 1: 2 \* (n/2) = n
* Level 2: 4 \* (n/4) = n
* ... log n levels
  Total cost = n \* log n → T(n) = O(n log n)

---

### Conclusion

This module provides the foundational knowledge for evaluating and comparing algorithms. It helps in choosing the best algorithm for a problem based on time and space efficiency. Key topics such as asymptotic notations, different cases of analysis, and recurrence solving methods are essential tools for algorithm designers and software engineers.

---

